{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TWICE dataset (Parsing OSI files)\n",
    "\n",
    "Author: Novicki Neto, Leonardo (2022)<br>\n",
    "CARISSMA Institute of Automated Driving (CIAD)<br>\n",
    "Technische Hochschule Ingolstadt (THI)<br>\n",
    "Federal University of Parana (UFPR)\n",
    "\n",
    "## Directory structure\n",
    "```\n",
    "TWICE/\n",
    "   Scenarios/                    # the full or partial dataset\n",
    "   open-simulation-interface/    # installed separately see below\n",
    "   scripts/                      # this example scripts\n",
    "      Results/                   # empty folder for results\n",
    "```   \n",
    "   \n",
    "## Requirements:\n",
    "1. pip install --upgrade opencv-python tqdm open3d\n",
    "2. Open simulation interface (OSI) - [Installation instructions](https://www.asam.net/static_downloads/ASAM_OSI_reference-documentation_v3.5.0/index.html)\n",
    "3. pip install --upgrade google-api-python-client\n",
    "\n",
    "### TWICE home page\n",
    "https://twice.eletrica.ufpr.br<br>\n",
    "https://twicedataset.github.io/site\n",
    "\n",
    "Reference:<br>\n",
    "L. Novicki Neto et al., “TWICE Dataset: Digital Twin of Test Scenarios in a Controlled Environment.” arXiv, Oct. 05, 2023. doi: 10.48550/arXiv.2310.03895."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "path_twice = \"..\"  # change if TWICE is in other directory\n",
    "\n",
    "import google.protobuf.message\n",
    "import google.protobuf.json_format\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import struct\n",
    "import math as m\n",
    "from cuboid_project import Camera_project\n",
    "import lidar_osi2PLY \n",
    "import radar_osi_plot\n",
    "import file_selector\n",
    "sys.path.insert(1,os.path.join(path_twice,\"open-simulation-interface\"))\n",
    "from osi3.osi_sensordata_pb2 import SensorData\n",
    "from osi3.osi_datarecording_pb2 import SensorDataSeries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code below you can select the desire scenario, with the different parameters. If you select a combination of parameters that was not recorded it will return an error. Any warnings about the scenario will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = \"daytime\" # daytime, rain, night, snow\n",
    "scenario = \"CCRb\" #CCRb, CCRs, car, bike, pedestrian, truck, truck_perpendicular, os.path.join(\"CBLA\",\"with_car\"), os.path.join(\"CBLA\",\"withot_car\")\n",
    "scenario_type = \"real\" # real, synthetic\n",
    "radar_mode = \"cluster\" # cluster, object_list\n",
    "test_run = 1 # Usually has 3 test_runs, but it varies from 1 to 4.\n",
    "camera_synthetic = \"DDI\" #If synthetic \"DDI\" for Direct Data Injection or \"OTA\" for Over-the-Air\n",
    "camera_path,radar_path,ego_path,obj_path,lidar_path = file_selector.file_selector(path_twice,scenario,weather,scenario_type,radar_mode,test_run, camera_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below open the image file: You can save all the test_run in a video, or just see a frame. You also have the option to project the ground truth cuboid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pathSaveImage=\"Results/\" #path to save image and video (with slash)\n",
    "frame=60\n",
    "showImage=False\n",
    "saveVideo=True\n",
    "saveImage=False\n",
    "projectCuboid=True\n",
    "showFrame=True\n",
    "showTimestamp=True\n",
    "Camera_project.project_cuboid_image(camera_path,frame,projectCuboid,showImage,saveVideo,saveImage,showFrame,showTimestamp,pathSaveImage)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code below you can generate a video file from the Radar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_radar_video = \"Results/\" #path of folder where you want to save the radar video file (with slash)\n",
    "save_video = True\n",
    "save_frame = True\n",
    "frame_n = 8\n",
    "count_points = True #Cluster mode: count the number of cluster points inside object ground truth\n",
    "covariance =  True # Plot covariance ellipses for cluster points\n",
    "IoU = True # Object List mode: calculate the Intersection over Union of the detect rectangle with object ground truth\n",
    "segmentation_data =  False #data from pc segmentation of the object\n",
    "text = True #text indicating ego and object next to their respective rectangles\n",
    "if radar_mode == \"object_list\":\n",
    "    # radar_osi_plot.radar_objlist_plot(radar_path,path_radar_video,save_video,save_frame,frame_n,IoU,segmentation_data,text)\n",
    "    radar_osi_plot.radar_objlist_plot(radar_path,path_radar_video,save_video,save_frame,frame_n,IoU,segmentation_data)\n",
    "if radar_mode == \"cluster\":\n",
    "    # radar_osi_plot.radar_cluster_plot(radar_path,path_radar_video,save_video,save_frame,frame_n,count_points,covariance,segmentation_data,text)\n",
    "    radar_osi_plot.radar_cluster_plot(radar_path,path_radar_video,save_video,save_frame,frame_n,count_points,covariance,segmentation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below open the compressed image and export with the correspondent timestamp [SensorData Documentation](https://www.asam.net/static_downloads/ASAM_OSI_reference-documentation_v3.5.0/structosi3_1_1SensorDataSeries.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = SensorData()\n",
    "img_path = \"Results/\" #folder that you want to export the images with slash\n",
    "count=0\n",
    "frames = []\n",
    "with open(camera_path,'rb') as f:\n",
    "    while 1:\n",
    "        message_size_bytes = f.read(struct.calcsize(\"<L\"))\n",
    "        if len(message_size_bytes) == 0:\n",
    "            break\n",
    "        message_size = struct.unpack(\"<L\",message_size_bytes)[0]\n",
    "        message_bytes = f.read(message_size)\n",
    "        img.ParseFromString(message_bytes)\n",
    "        str_msg =img.sensor_view[0].camera_sensor_view[0].image_data\n",
    "        timestamp = (img.sensor_view[0].timestamp.seconds)+(img.sensor_view[0].timestamp.nanos/1000000000)\n",
    "        name_path=img_path+str(count)+\"_\"+str(timestamp)+\".png\"\n",
    "        cv_img = np.ndarray(shape=(1, len(str_msg)), dtype=np.uint8, buffer=str_msg)\n",
    "        im = cv2.imdecode(cv_img, cv2.IMREAD_ANYCOLOR)\n",
    "        cv2.imwrite(name_path,im)\n",
    "        count+=1\n",
    "     \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the LiDAR data. The LiDAR data is serialized using the struct package, just like the camera data. The code below read only the first detection of the LiDAR, to read all the data just erase the break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lidar_obj = SensorData()\n",
    "# with open(lidar_path,'rb') as f:\n",
    "#     while 1:\n",
    "#         message_size_bytes = f.read(struct.calcsize(\"<L\"))\n",
    "#         if len(message_size_bytes) == 0:\n",
    "#             break\n",
    "#         message_size = struct.unpack(\"<L\",message_size_bytes)[0]\n",
    "#         message_bytes = f.read(message_size)\n",
    "#         lidar_obj.ParseFromString(message_bytes)\n",
    "#         print(lidar_obj.feature_data.lidar_sensor[0])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to get a parameter from the lidar_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lidar_obj.feature_data.lidar_sensor[0].detection[0].intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanna convert the LiDAR data from .osi to .ply, just run the code below. It will generate one .ply file for each timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path_ply_files = \"/path/to/save\" #path of folder where you want to save the .ply files\n",
    "# path_ply_files = \"Results/\" #path of folder where you want to save the .ply files\n",
    "# lidar_osi2PLY.osi2ply(lidar_path,path_ply_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open .osi SensorDataSeries(ADMA, GPS and radar data) [Documentation](https://www.asam.net/static_downloads/ASAM_OSI_reference-documentation_v3.5.0/structosi3_1_1SensorDataSeries.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_ego = SensorDataSeries()\n",
    "path_osi_file = ego_path\n",
    "with open(path_osi_file,'rb') as f:\n",
    "    obj_ego.ParseFromString(f.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print an object of SensorDataSeries you can visualize how the data is organized inside OSI class structure. It is also useful see the diagrams that are avaible in the [OSI documentation](https://www.asam.net/static_downloads/ASAM_OSI_reference-documentation_v3.5.0/structosi3_1_1RadarDetectionData.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp {\n",
       "  seconds: 0\n",
       "  nanos: 0\n",
       "}\n",
       "host_vehicle_data {\n",
       "  location {\n",
       "    dimension {\n",
       "      length: 2.695\n",
       "      width: 1.559\n",
       "      height: 1.565\n",
       "    }\n",
       "  }\n",
       "  vehicle_localization {\n",
       "    geodetic_position {\n",
       "      longitude: 11.471423149108887\n",
       "      latitude: 48.78376007080078\n",
       "      altitude: 372.1000061035156\n",
       "    }\n",
       "  }\n",
       "  vehicle_motion {\n",
       "    position {\n",
       "      x: -128.42999267578125\n",
       "      y: -94.86000061035156\n",
       "    }\n",
       "    orientation {\n",
       "      roll: -0.017104227002439566\n",
       "      pitch: -0.0024434609631950343\n",
       "      yaw: 5.27595594499068\n",
       "    }\n",
       "    velocity {\n",
       "      x: 0.08500000089406967\n",
       "      y: 0.004999999888241291\n",
       "      z: 0.0\n",
       "    }\n",
       "    orientation_rate {\n",
       "      roll: 0.0012217304815975172\n",
       "      pitch: 0.0006981316851932723\n",
       "      yaw: -0.0008726646390008811\n",
       "    }\n",
       "    acceleration {\n",
       "      x: 0.03138127920724219\n",
       "      y: -0.1882876889431849\n",
       "      z: 9.826263047486544\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(obj_ego.sensor_data[0].sensor_view[0].global_ground_truth.stationary_object[0].base.position.x)\n",
    "obj_ego.sensor_data[0].sensor_view[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp {\n",
       "  seconds: 0\n",
       "  nanos: 0\n",
       "}\n",
       "host_vehicle_data {\n",
       "  location {\n",
       "    dimension {\n",
       "      length: 2.695\n",
       "      width: 1.559\n",
       "      height: 1.565\n",
       "    }\n",
       "  }\n",
       "  vehicle_localization {\n",
       "    geodetic_position {\n",
       "      longitude: 11.471423149108887\n",
       "      latitude: 48.78376007080078\n",
       "      altitude: 372.1000061035156\n",
       "    }\n",
       "  }\n",
       "  vehicle_motion {\n",
       "    position {\n",
       "      x: -128.42999267578125\n",
       "      y: -94.86000061035156\n",
       "    }\n",
       "    orientation {\n",
       "      roll: -0.017104227002439566\n",
       "      pitch: -0.0024434609631950343\n",
       "      yaw: 5.27595594499068\n",
       "    }\n",
       "    velocity {\n",
       "      x: 0.08500000089406967\n",
       "      y: 0.004999999888241291\n",
       "      z: 0.0\n",
       "    }\n",
       "    orientation_rate {\n",
       "      roll: 0.0012217304815975172\n",
       "      pitch: 0.0006981316851932723\n",
       "      yaw: -0.0008726646390008811\n",
       "    }\n",
       "    acceleration {\n",
       "      x: 0.03138127920724219\n",
       "      y: -0.1882876889431849\n",
       "      z: 9.826263047486544\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_ego.sensor_data[0].sensor_view[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to acess the .osi data from the ego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_position_x=[]\n",
    "ego_position_y=[]\n",
    "ego_time=[]\n",
    "for ind in range(len(obj_ego.sensor_data)):\n",
    "    ego_position_x.append(obj_ego.sensor_data[ind].sensor_view[0].host_vehicle_data.vehicle_motion.position.x)\n",
    "    ego_position_y.append(obj_ego.sensor_data[ind].sensor_view[0].host_vehicle_data.vehicle_motion.position.y)\n",
    "    ego_time.append((obj_ego.sensor_data[ind].sensor_view[0].timestamp.seconds)+(obj_ego.sensor_data[ind].sensor_view[0].timestamp.nanos/1000000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_radar = SensorDataSeries()\n",
    "with open(radar_path,'rb') as f:\n",
    "    obj_radar.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acess radar data, the sensor data index changes for each timestamp. Each cluster for that timestamp is inside detection list. In the example below, the cluster_rcs variable receives the rcs value for the first timestamp detection and the first cluster within that timestamp [RadarDetectionData Documentation](https://www.asam.net/static_downloads/ASAM_OSI_reference-documentation_v3.5.0/structosi3_1_1RadarDetectionData.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5\n",
      "-12.5\n",
      "-2.5\n",
      "10.5\n",
      "-12.0\n",
      "-1.5\n",
      "8.5\n",
      "-1.5\n",
      "-12.0\n",
      "8.0\n",
      "-1.5\n",
      "-12.0\n"
     ]
    }
   ],
   "source": [
    "iframe=0\n",
    "for frame in range(len(obj_radar.sensor_data)):\n",
    "    iind=0\n",
    "    for ind in range(len(obj_radar.sensor_data[frame].feature_data.radar_sensor[0].detection)): \n",
    "        rcs = obj_radar.sensor_data[frame].feature_data.radar_sensor[0].detection[ind].rcs\n",
    "        print(rcs)\n",
    "        iind +=1\n",
    "        if iind==3:\n",
    "            break\n",
    "    iframe +=1\n",
    "    if iframe==4:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in range(len(obj_radar.sensor_data)):\n",
    "    for ind in range(len(obj_radar.sensor_data[frame].moving_object)):\n",
    "        prob_exist= obj_radar.sensor_data[frame].moving_object[ind].candidate[0].probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angle_obj = []\n",
    "# for frame in range(len(obj_radar.sensor_data)):\n",
    "#     for ind in range(len(obj_radar.sensor_data[frame].moving_object)):\n",
    "#         obj_det_angle = obj_radar.sensor_data[frame].moving_object[ind].base.orientation.yaw\n",
    "#         angle_obj.append(obj_det_angle)\n",
    "# print(m.degrees(max(angle_obj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert SensorDataSeries into .json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=google.protobuf.json_format.MessageToJson(obj_radar)\n",
    "with open(\"Results/teste_ego_sim_series.json\",'w') as outfile:\n",
    "    outfile.write(obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
