{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing OSI files\n",
    "Author: Novicki Neto, Leonardo (2022)\n",
    "\n",
    "CARISSMA Institute of Automated Driving (CIAD)\n",
    "\n",
    "Technische Hochschule Ingolstadt (THI)\n",
    "## Requirements:\n",
    "Follow instructions of OSI Installation [available at here](https://www.asam.net/static_downloads/ASAM_OSI_reference-documentation_v3.5.0/index.html)\n",
    "1. pip install --upgrade google-api-python-client\n",
    "\n",
    "## Important:\n",
    "You need to change the path on the TWICE_path.txt in order to this script works (path where the folder TWICE is stored) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TWICE_path.txt', 'r') as f:\n",
    "    path_twice = f.read()\n",
    "import google.protobuf.message\n",
    "import google.protobuf.json_format\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import struct\n",
    "import math as m\n",
    "from cuboid_project import Camera_project\n",
    "import lidar_osi2PLY \n",
    "import radar_osi_plot\n",
    "import file_selector\n",
    "sys.path.insert(1,os.path.join(path_twice,\"TWICE\",\"open-simulation-interface\"))\n",
    "from osi3.osi_sensordata_pb2 import SensorData\n",
    "from osi3.osi_datarecording_pb2 import SensorDataSeries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code below you can select the desire scenario, with the different parameters. If you select a combination of parameters that was not recorded it will return an error. Any warnings about the scenario will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = \"daytime\" # daytime, rain, night, snow\n",
    "scenario = \"car\" #CCRb, CCRs, car, bike, pedestrian, truck, truck_perpendicular, os.path.join(\"CBLA\",\"with_car\"), os.path.join(\"CBLA\",\"withot_car\")\n",
    "scenario_type = \"real\" # real, synthetic\n",
    "radar_mode = \"object_list\" # cluster, object_list\n",
    "test_run = 2 # Usually has 3 test_runs, but it varies from 1 to 4.\n",
    "camera_synthetic = \"DDI\" #If synthetic \"DDI\" for Direct Data Injection or \"OTA\" for Over-the-Air\n",
    "camera_path,radar_path,ego_path,obj_path,lidar_path = file_selector.file_selector(scenario,weather,scenario_type,radar_mode,test_run, camera_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below open the image file: You can save all the test_run in a video, or just see a frame. You also have the option to project the ground truth cuboid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting cuboid:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443/443 [00:06<00:00, 73.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing video file:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443/443 [00:07<00:00, 62.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pathSaveImage=\"/path/to/save\" #path to save image and video\n",
    "frame=60\n",
    "showImage=False\n",
    "saveVideo=True\n",
    "saveImage=False\n",
    "projectCuboid=True\n",
    "showFrame=True\n",
    "showTimestamp=True\n",
    "Camera_project.project_cuboid_image(camera_path,frame,projectCuboid,showImage,saveVideo,saveImage,showFrame,showTimestamp,pathSaveImage)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code below you can generate a video file from the Radar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_radar_video = \"/path/to/save\" #path of folder where you want to save the radar video file\n",
    "save_video = True\n",
    "save_frame = True\n",
    "frame_n = 8\n",
    "count_points = True #Cluster mode: count the number of cluster points inside object ground truth\n",
    "covariance =  True # Plot covariance ellipses for cluster points\n",
    "IoU = True # Object List mode: calculate the Intersection over Union of the detect rectangle with object ground truth\n",
    "segmentation_data =  False #data from pc segmentation of the object\n",
    "text = True #text indicating ego and object next to their respective rectangles\n",
    "if radar_mode == \"object_list\":\n",
    "    radar_osi_plot.radar_objlist_plot(radar_path,path_radar_video,save_video,save_frame,frame_n,IoU,segmentation_data,text)\n",
    "if radar_mode == \"cluster\":\n",
    "    radar_osi_plot.radar_cluster_plot(radar_path,path_radar_video,save_video,save_frame,frame_n,count_points,covariance,segmentation_data,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below open the compressed image and export with the correspondent timestamp [SensorData Documentation](https://www.asam.net/static_downloads/ASAM_OSI_reference-documentation_v3.5.0/structosi3_1_1SensorDataSeries.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = SensorData()\n",
    "img_path = \"/path/to/save\" #folder that you want to export the images\n",
    "count=0\n",
    "frames = []\n",
    "with open(camera_path,'rb') as f:\n",
    "    while 1:\n",
    "        message_size_bytes = f.read(struct.calcsize(\"<L\"))\n",
    "        if len(message_size_bytes) == 0:\n",
    "            break\n",
    "        message_size = struct.unpack(\"<L\",message_size_bytes)[0]\n",
    "        message_bytes = f.read(message_size)\n",
    "        img.ParseFromString(message_bytes)\n",
    "        str_msg =img.sensor_view[0].camera_sensor_view[0].image_data\n",
    "        timestamp = (img.sensor_view[0].timestamp.seconds)+(img.sensor_view[0].timestamp.nanos/1000000000)\n",
    "        name_path=img_path+str(count)+\"_\"+str(timestamp)+\".png\"\n",
    "        cv_img = np.ndarray(shape=(1, len(str_msg)), dtype=np.uint8, buffer=str_msg)\n",
    "        im = cv2.imdecode(cv_img, cv2.IMREAD_ANYCOLOR)\n",
    "        cv2.imwrite(name_path,im)\n",
    "        count+=1\n",
    "     \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the LiDAR data. The LiDAR data is serialized using the struct package, just like the camera data. The code below read only the first detection of the LiDAR, to read all the data just erase the break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_obj = SensorData()\n",
    "with open(lidar_path,'rb') as f:\n",
    "    while 1:\n",
    "        message_size_bytes = f.read(struct.calcsize(\"<L\"))\n",
    "        if len(message_size_bytes) == 0:\n",
    "            break\n",
    "        message_size = struct.unpack(\"<L\",message_size_bytes)[0]\n",
    "        message_bytes = f.read(message_size)\n",
    "        lidar_obj.ParseFromString(message_bytes)\n",
    "        print(lidar_obj.feature_data.lidar_sensor[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to get a parameter from the lidar_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lidar_obj.feature_data.lidar_sensor[0].detection[0].intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanna convert the LiDAR data from .osi to .ply, just run the code below. It will generate one .ply file for each timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ply_files = \"/path/to/save\" #path of folder where you want to save the .ply files\n",
    "lidar_osi2PLY.osi2ply(lidar_path,path_ply_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open .osi SensorDataSeries(ADMA, GPS and radar data) [Documentation](https://www.asam.net/static_downloads/ASAM_OSI_reference-documentation_v3.5.0/structosi3_1_1SensorDataSeries.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_ego = SensorDataSeries()\n",
    "path_osi_file = ego_path\n",
    "with open(path_osi_file,'rb') as f:\n",
    "    obj_ego.ParseFromString(f.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print an object of SensorDataSeries you can visualize how the data is organized inside OSI class structure. It is also useful see the diagrams that are avaible in the [OSI documentation](https://www.asam.net/static_downloads/ASAM_OSI_reference-documentation_v3.5.0/structosi3_1_1RadarDetectionData.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obj_ego.sensor_data[0].sensor_view[0].global_ground_truth.stationary_object[0].base.position.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to acess the .osi data from the ego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_position_x=[]\n",
    "ego_position_y=[]\n",
    "ego_time=[]\n",
    "for ind in range(len(obj_ego.sensor_data)):\n",
    "    ego_position_x.append(obj_ego.sensor_data[ind].sensor_view[0].host_vehicle_data.vehicle_motion.position.x)\n",
    "    ego_position_y.append(obj_ego.sensor_data[ind].sensor_view[0].host_vehicle_data.vehicle_motion.position.y)\n",
    "    ego_time.append((obj_ego.sensor_data[ind].sensor_view[0].timestamp.seconds)+(obj_ego.sensor_data[ind].sensor_view[0].timestamp.nanos/1000000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_radar = SensorDataSeries()\n",
    "with open(radar_path,'rb') as f:\n",
    "    obj_radar.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acess radar data, the sensor data index changes for each timestamp. Each cluster for that timestamp is inside detection list. In the example below, the cluster_rcs variable receives the rcs value for the first timestamp detection and the first cluster within that timestamp [RadarDetectionData Documentation](https://www.asam.net/static_downloads/ASAM_OSI_reference-documentation_v3.5.0/structosi3_1_1RadarDetectionData.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in range(len(obj_radar.sensor_data)):\n",
    "    for ind in range(len(obj_radar.sensor_data[frame].feature_data.radar_sensor[0].detection)): \n",
    "        rcs = obj_radar.sensor_data[frame].feature_data.radar_sensor[0].detection[ind].rcs\n",
    "        print(rcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for frame in range(len(obj_radar.sensor_data)):\n",
    "    for ind in range(len(obj_radar.sensor_data[frame].moving_object)):\n",
    "        prob_exist= obj_radar.sensor_data[frame].moving_object[ind].candidate[0].probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_obj = []\n",
    "for frame in range(len(obj_radar.sensor_data)):\n",
    "    for ind in range(len(obj_radar.sensor_data[frame].moving_object)):\n",
    "        obj_det_angle = obj_radar.sensor_data[frame].moving_object[ind].base.orientation.yaw\n",
    "        angle_obj.append(obj_det_angle)\n",
    "print(m.degrees(max(angle_obj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert SensorDataSeries into .json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=google.protobuf.json_format.MessageToJson(obj_radar)\n",
    "with open(\"teste_ego_sim_series.json\",'w') as outfile:\n",
    "    outfile.write(obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
